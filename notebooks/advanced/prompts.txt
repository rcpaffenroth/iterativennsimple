## Objective
Create notebook: `notebooks/advanced/6-rcp-Sequential2D-training-spectra.ipynb`

## Problem Description
Train two Sequential2D models on a simple regression task: predict the Euclidean norm (L2 length) of an input vector x.
The goal is to visualize how the singular value spectra of the weight matrices in each block evolve during training.

## Task Requirements

### 1. Dataset
- Input: random vectors x ∈ ℝ^d (e.g., d=8)
- Output: scalar y = ||x||₂ (Euclidean norm)
- Generate synthetic training and validation sets using torch.randn()

### 2. Model Architecture Details

**Model 1: Chain Architecture (superdiagonal blocks)**
```python
# Define dimensions
input_size = 8
hidden_size_1 = 16
hidden_size_2 = 16  
output_size = 1

# Create MaskedLinear layers - these are the trainable weight matrices we'll analyze
f1_layer = MaskedLinear(in_features=input_size, out_features=hidden_size_1, bias=False)
f2_layer = MaskedLinear(in_features=hidden_size_1, out_features=hidden_size_2, bias=False)
f3_layer = MaskedLinear(in_features=hidden_size_2, out_features=output_size, bias=False)

# Wrap in Sequential1D blocks (F1 has no activation before it)
F1 = Sequential1D(nn.Sequential(f1_layer), 
                  in_features=input_size, out_features=hidden_size_1)
F2 = Sequential1D(nn.Sequential(nn.ReLU(), f2_layer), 
                  in_features=hidden_size_1, out_features=hidden_size_2)
F3 = Sequential1D(nn.Sequential(nn.ReLU(), f3_layer), 
                  in_features=hidden_size_2, out_features=output_size)

# Build 4x4 block matrix with chain structure (only superdiagonal filled)
in_features_list = [input_size, hidden_size_1, hidden_size_2, output_size]
out_features_list = [input_size, hidden_size_1, hidden_size_2, output_size]

blocks = [[None, F1,   None, None],
          [None, None, F2,   None],
          [None, None, None, F3  ],
          [None, None, None, None]]

model1 = Sequential2D(blocks, in_features_list, out_features_list)
```

**Model 2: Sparse MaskedLinear Architecture (all lateral connections)**
```python
# Create 12 MaskedLinear layers for all non-None blocks
g01_layer = MaskedLinear(in_features=input_size, out_features=hidden_size_1, bias=False)
g02_layer = MaskedLinear(in_features=input_size, out_features=hidden_size_2, bias=False)
# ... etc for g03, g11, g12, g13, g21, g22, g23, g31, g32, g33

# Wrap each in Sequential1D (first column has no activation, others have ReLU)
G01 = Sequential1D(nn.Sequential(g01_layer), 
                   in_features=input_size, out_features=hidden_size_1)
G11 = Sequential1D(nn.Sequential(nn.ReLU(), g11_layer), 
                   in_features=hidden_size_1, out_features=hidden_size_1)
# ... etc for all Gij

# Build 4x4 block matrix with dense structure
blocks = [[I,    G01, G02, G03],
          [None, G11, G12, G13],
          [None, G21, G22, G23],
          [None, G31, G32, G33]]

model2 = Sequential2D(blocks, in_features_list, out_features_list)
```
 Specifically, the block sizes are:
- G01: (input_size, hidden_size_1)
- G02: (input_size, hidden_size_2)
- G03: (input_size, output_size)
- G11: (hidden_size_1, hidden_size_1)
- G12: (hidden_size_1, hidden_size_2)
- G13: (hidden_size_1, output_size)
- G21: (hidden_size_2, hidden_size_1)
- G22: (hidden_size_2, hidden_size_2)
- G23: (hidden_size_2, output_size)
- G31: (output_size, hidden_size_1)
- G32: (output_size, hidden_size_2)
- G33: (output_size, output_size)

### 3. Training Procedure
- Use MSE loss (regression task)
- Adam optimizer (learning rate ~0.001)
- Train for enough epochs to see some convergence but run quickly (e.g., 20-50 epochs)
- model1 must be iterated over 3 time steps, model2 over 4 time steps

### 4. Singular Value Analysis & Visualization

For each model, at each checkpoint:
- Extract the actual weight matrix from each MaskedLinear layer
  - Access via: `layer.weight_0 + layer.U * layer.mask` (this gives the full trained weight)
- Compute SVD using `torch.linalg.svd()` to get singular values for each block
- Create plots showing:
  - Singular value spectra (bar plots or line plots) for each block
  - Evolution of singular values over training epochs (line plots)
  - Also, create plots for the sorted union of singular values across all blocks for each model at each epoch

Example visualization structure:
- Figure 1: Model 1 spectra evolution (one subplot per block F1, F2, F3)
- Figure 2: Model 2 spectra evolution (grid of subplots for all Gij blocks)
- Figure 3: Comparison of spectral properties (e.g., condition numbers, rank approximations)

### 5. Code Style Guidelines
- Follow the style of `notebooks/6-rcp-Sequential-vs-Sequential2D.ipynb`
- Use clear variable names and extensive markdown documentation
- Keep functions simple and focused (avoid over-engineering)
- Add comments explaining the mathematical concepts
- Make hyperparameters easy to modify (define at top of cells)
- Use reproducible random seeds

### 6. Key Implementation Notes
- MaskedLinear stores weights as: `weight = weight_0 + U * mask`
- Access the full weight matrix for SVD analysis using the property above
- Sequential1D is just a wrapper - the actual linear transformation is in the MaskedLinear
- Sequential2D.blocks is a 4x4 matrix where None represents no connection
- The in_features_list and out_features_list define dimensions at each "time step" in the iteration

### Expected Output Structure
1. **Markdown cell**: Problem description and mathematical setup
2. **Code cells**: Imports, data generation, model definitions
3. **Code cells**: Training loops with checkpoint saving
4. **Code cells**: SVD computation functions
5. **Visualization cells**: Multiple figures showing spectral evolution
6. **Markdown cells**: Analysis and interpretation of results

## Reference Materials
- Study `notebooks/6-rcp-Sequential-vs-Sequential2D.ipynb` for coding style and structure
- See `iterativennsimple/MaskedLinear.py` for weight matrix structure
- See `iterativennsimple/Sequential2D.py` for block matrix architecture