{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4475020",
   "metadata": {},
   "source": [
    "# Sequential vs Sequential2D Comparison\n",
    "\n",
    "This notebook compares PyTorch's built-in Sequential container with our custom Sequential2D container, demonstrating:\n",
    "\n",
    "1. **Functional Equivalence**: Cases where both produce identical results\n",
    "2. **Architectural Differences**: Unique capabilities of Sequential2D\n",
    "3. **Performance Analysis**: Memory and computational efficiency comparisons\n",
    "4. **Use Case Scenarios**: When to choose each approach\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the differences between linear sequential architectures and 2D block architectures\n",
    "- Learn when Sequential2D provides advantages over standard Sequential\n",
    "- Analyze performance trade-offs in different scenarios\n",
    "- Gain insights into advanced neural network architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba40f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from iterativennsimple.Sequential2D import Sequential2D, Identity\n",
    "from iterativennsimple.Sequential1D import Sequential1D\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828cc19",
   "metadata": {},
   "source": [
    "## Part 1: Functional Equivalence\n",
    "\n",
    "Let's start by showing how Sequential2D can replicate the behavior of PyTorch's Sequential container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02c35e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical number of parameters:118282\n",
      "Network architectures created successfully!\n",
      "Sequential parameters: 118282\n",
      "Sequential2D parameters: 118282\n"
     ]
    }
   ],
   "source": [
    "def create_equivalent_networks():\n",
    "    \"\"\"\n",
    "    Create functionally equivalent networks using Sequential and Sequential2D\n",
    "    \"\"\"\n",
    "    # Network dimensions\n",
    "    input_size = 784  # MNIST-like input\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    \n",
    "    f1 = nn.Linear(input_size, hidden_size)\n",
    "    f2 = nn.Linear(hidden_size, hidden_size)\n",
    "    f3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # PyTorch Sequential Network\n",
    "    sequential_net = nn.Sequential(\n",
    "        f1,\n",
    "        nn.ReLU(),\n",
    "        f2,\n",
    "        nn.ReLU(),\n",
    "        f3\n",
    "    )\n",
    "    \n",
    "    in_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "    out_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "    \n",
    "    # Create blocks matrix for linear chain\n",
    "    I = Identity(in_features=input_size, out_features=input_size)\n",
    "    # Note, the ReLU activations are included in the Sequential1D blocks, but can go different places\n",
    "    # in the Sequential2D structure.\n",
    "    F1 = Sequential1D(nn.Sequential(f1),            in_features=input_size,  out_features=hidden_size)\n",
    "    F2 = Sequential1D(nn.Sequential(nn.ReLU(), f2), in_features=hidden_size, out_features=hidden_size)\n",
    "    F3 = Sequential1D(nn.Sequential(nn.ReLU(), f3), in_features=hidden_size, out_features=output_size)\n",
    "\n",
    "    # NOTE: Mind the transposed structure of the blocks matrix!\n",
    "    blocks = [[I,    F1,   None, None],\n",
    "              [None, None, F2,   None],\n",
    "              [None, None, None, F3],\n",
    "              [None, None, None, None]]\n",
    "    W_parameters = input_size * hidden_size + hidden_size * hidden_size + hidden_size * output_size \n",
    "    b_parameters = hidden_size + hidden_size + output_size\n",
    "    print(f\"Theoretical number of parameters:{W_parameters + b_parameters}\")\n",
    "    sequential2d_net = Sequential2D(in_features_list, out_features_list, blocks)    \n",
    "    return sequential_net, sequential2d_net\n",
    "\n",
    "# Create equivalent networks\n",
    "seq_net, seq2d_net = create_equivalent_networks()\n",
    "\n",
    "print(\"Network architectures created successfully!\")\n",
    "print(f\"Sequential parameters: {sum(p.numel() for p in seq_net.parameters())}\")\n",
    "print(f\"Sequential2D parameters: {sum(p.numel() for p in seq2d_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc52be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between outputs: 0.00e+00\n",
      "Outputs are identical\n"
     ]
    }
   ],
   "source": [
    "def test_equivalence():\n",
    "    \"\"\"\n",
    "    Test that both networks produce identical outputs\n",
    "    \"\"\"\n",
    "    # Create test input\n",
    "    batch_size = 32\n",
    "    input_size = 784\n",
    "    test_input = torch.randn(batch_size, input_size)\n",
    "    \n",
    "    # Get outputs from both networks\n",
    "    with torch.no_grad():\n",
    "        # Sequential network forward pass\n",
    "        seq_output = seq_net(test_input)\n",
    "        \n",
    "        seq2d_output = [test_input, None, None, None]\n",
    "\n",
    "        # Here is the magic! You iterate a *fixed* function (seq2d_net) on the input x.  Though the magic of linear\n",
    "        # algebra, this is equivalent to the sequential network.\n",
    "        for i in range(3):\n",
    "            seq2d_output = seq2d_net(seq2d_output)\n",
    "    # Check if outputs are identical.\n",
    "    # Note: the \"output\" of the Sequential2D is a list, where the last element is the output for this particular structure.\n",
    "    max_diff = torch.max(torch.abs(seq_output - seq2d_output[3])).item()\n",
    "    \n",
    "    print(f\"Maximum difference between outputs: {max_diff:.2e}\")\n",
    "    print(f\"Outputs are {'identical' if max_diff < 1e-6 else 'different'}\")\n",
    "    \n",
    "    return seq_output, seq2d_output\n",
    "\n",
    "seq_output, seq2d_output = test_equivalence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6baff",
   "metadata": {},
   "source": [
    "# NOT DONE BELOW HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff06a1",
   "metadata": {},
   "source": [
    "## Part 2: Unique Capabilities of Sequential2D\n",
    "\n",
    "Now let's explore scenarios where Sequential2D offers capabilities that standard Sequential cannot provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc2686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Sequential2D Network Architecture:\n",
      "- Multiple parallel paths from input to output\n",
      "- Skip connections across multiple layers\n",
      "- Aggregation of features at different scales\n",
      "- Total parameters: 363550\n"
     ]
    }
   ],
   "source": [
    "def create_complex_sequential2d():\n",
    "    \"\"\"\n",
    "    Create a Sequential2D network with complex connectivity patterns\n",
    "    that cannot be represented by standard Sequential\n",
    "    \"\"\"\n",
    "    # Multi-path architecture with skip connections and parallel processing\n",
    "    in_features_list = [784, 256, 128, 64]   # Multiple input paths\n",
    "    out_features_list = [256, 128, 64, 10]   # Multiple output paths\n",
    "    \n",
    "    # Create a complex connectivity pattern\n",
    "    blocks = [\n",
    "        # Row 0: Input layer connections\n",
    "        [nn.Linear(784, 256), nn.Linear(784, 128), None, None],\n",
    "        \n",
    "        # Row 1: First hidden layer connections  \n",
    "        [None, nn.Linear(256, 128), nn.Linear(256, 64), nn.Linear(256, 10)],\n",
    "        \n",
    "        # Row 2: Second hidden layer connections\n",
    "        [None, None, nn.Linear(128, 64), nn.Linear(128, 10)],\n",
    "        \n",
    "        # Row 3: Third hidden layer connections\n",
    "        [None, None, None, nn.Linear(64, 10)]\n",
    "    ]\n",
    "    \n",
    "    complex_net = Sequential2D(in_features_list, out_features_list, blocks)\n",
    "    \n",
    "    print(\"Complex Sequential2D Network Architecture:\")\n",
    "    print(\"- Multiple parallel paths from input to output\")\n",
    "    print(\"- Skip connections across multiple layers\") \n",
    "    print(\"- Aggregation of features at different scales\")\n",
    "    print(f\"- Total parameters: {sum(p.numel() for p in complex_net.parameters())}\")\n",
    "    \n",
    "    return complex_net\n",
    "\n",
    "complex_net = create_complex_sequential2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53962ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The input has the wrong number of features. (torch.Size([16, 784]), 1232)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, output_list\n\u001b[0;32m---> 27\u001b[0m output, output_list \u001b[38;5;241m=\u001b[39m \u001b[43mdemonstrate_complex_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mdemonstrate_complex_forward\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m784\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass through complex network\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcomplex_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/2_research/iterativennsimple/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/2_research/iterativennsimple/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/2_research/iterativennsimple/iterativennsimple/Sequential2D.py:81\u001b[0m, in \u001b[0;36mSequential2D.forward\u001b[0;34m(self, X_in)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_list(X_in)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/2_research/iterativennsimple/iterativennsimple/Sequential2D.py:111\u001b[0m, in \u001b[0;36mSequential2D.forward_vector\u001b[0;34m(self, X_in)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_in):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m X_in\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe input has the wrong number of features. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_in\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    112\u001b[0m     X_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((X_in\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features), device\u001b[38;5;241m=\u001b[39mX_in\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features_list)):\n",
      "\u001b[0;31mAssertionError\u001b[0m: The input has the wrong number of features. (torch.Size([16, 784]), 1232)"
     ]
    }
   ],
   "source": [
    "def demonstrate_complex_forward():\n",
    "    \"\"\"\n",
    "    Demonstrate the complex forward pass of Sequential2D\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "    test_input = torch.randn(batch_size, 784)\n",
    "    \n",
    "    # Forward pass through complex network\n",
    "    output = complex_net(test_input)\n",
    "    \n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Let's also try the list-based forward pass\n",
    "    input_list = [test_input, None, None, None]  # Only provide input to first path\n",
    "    output_list = complex_net.forward_list(input_list)\n",
    "    \n",
    "    print(f\"\\nList-based forward pass:\")\n",
    "    for i, out in enumerate(output_list):\n",
    "        if out is not None:\n",
    "            print(f\"Output {i} shape: {out.shape}\")\n",
    "        else:\n",
    "            print(f\"Output {i}: None\")\n",
    "    \n",
    "    return output, output_list\n",
    "\n",
    "output, output_list = demonstrate_complex_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6863c",
   "metadata": {},
   "source": [
    "## Part 3: Performance Comparison\n",
    "\n",
    "Let's analyze the computational and memory efficiency of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_networks():\n",
    "    \"\"\"\n",
    "    Benchmark computational performance of Sequential vs Sequential2D\n",
    "    \"\"\"\n",
    "    # Test different network sizes\n",
    "    sizes = [\n",
    "        (256, 128, 64),\n",
    "        (512, 256, 128), \n",
    "        (1024, 512, 256),\n",
    "        (2048, 1024, 512)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for input_size, hidden_size, output_size in sizes:\n",
    "        print(f\"\\nTesting size: {input_size} -> {hidden_size} -> {output_size}\")\n",
    "        \n",
    "        # Create Sequential network\n",
    "        seq_net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create equivalent Sequential2D network\n",
    "        in_features_list = [input_size, hidden_size]\n",
    "        out_features_list = [hidden_size, output_size]\n",
    "        blocks = [\n",
    "            [nn.Linear(input_size, hidden_size), None],\n",
    "            [None, nn.Linear(hidden_size, output_size)]\n",
    "        ]\n",
    "        seq2d_net = Sequential2D(in_features_list, out_features_list, blocks).to(device)\n",
    "        \n",
    "        # Create test data\n",
    "        batch_size = 64\n",
    "        test_input = torch.randn(batch_size, input_size).to(device)\n",
    "        \n",
    "        # Benchmark Sequential\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(100):\n",
    "            with torch.no_grad():\n",
    "                _ = seq_net(test_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        seq_time = time.time() - start_time\n",
    "        \n",
    "        # Benchmark Sequential2D  \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(100):\n",
    "            with torch.no_grad():\n",
    "                x = test_input\n",
    "                h = seq2d_net.blocks['(0, 0)'](x)\n",
    "                h = torch.relu(h)\n",
    "                _ = seq2d_net.blocks['(1, 1)'](h)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        seq2d_time = time.time() - start_time\n",
    "        \n",
    "        # Memory usage\n",
    "        seq_params = sum(p.numel() for p in seq_net.parameters())\n",
    "        seq2d_params = sum(p.numel() for p in seq2d_net.parameters())\n",
    "        \n",
    "        results.append({\n",
    "            'input_size': input_size,\n",
    "            'sequential_time': seq_time,\n",
    "            'sequential2d_time': seq2d_time,\n",
    "            'sequential_params': seq_params,\n",
    "            'sequential2d_params': seq2d_params,\n",
    "            'speedup': seq_time / seq2d_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Sequential time: {seq_time:.4f}s\")\n",
    "        print(f\"Sequential2D time: {seq2d_time:.4f}s\") \n",
    "        print(f\"Speedup: {seq_time/seq2d_time:.2f}x\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_networks()\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249de98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison():\n",
    "    \"\"\"\n",
    "    Visualize performance comparison results\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Execution time comparison\n",
    "    ax1.plot(benchmark_results['input_size'], benchmark_results['sequential_time'], \n",
    "             'b-o', label='Sequential', linewidth=2, markersize=6)\n",
    "    ax1.plot(benchmark_results['input_size'], benchmark_results['sequential2d_time'], \n",
    "             'r-s', label='Sequential2D', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Input Size')\n",
    "    ax1.set_ylabel('Execution Time (s)')\n",
    "    ax1.set_title('Execution Time Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Speedup\n",
    "    ax2.plot(benchmark_results['input_size'], benchmark_results['speedup'], \n",
    "             'g-^', linewidth=2, markersize=6)\n",
    "    ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Input Size') \n",
    "    ax2.set_ylabel('Speedup (Sequential / Sequential2D)')\n",
    "    ax2.set_title('Performance Speedup')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Parameter count comparison\n",
    "    ax3.plot(benchmark_results['input_size'], benchmark_results['sequential_params'], \n",
    "             'b-o', label='Sequential', linewidth=2, markersize=6)\n",
    "    ax3.plot(benchmark_results['input_size'], benchmark_results['sequential2d_params'], \n",
    "             'r-s', label='Sequential2D', linewidth=2, markersize=6)\n",
    "    ax3.set_xlabel('Input Size')\n",
    "    ax3.set_ylabel('Number of Parameters')\n",
    "    ax3.set_title('Parameter Count Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Memory efficiency\n",
    "    memory_ratio = benchmark_results['sequential2d_params'] / benchmark_results['sequential_params']\n",
    "    ax4.plot(benchmark_results['input_size'], memory_ratio, \n",
    "             'purple', linewidth=2, marker='d', markersize=6)\n",
    "    ax4.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    ax4.set_xlabel('Input Size')\n",
    "    ax4.set_ylabel('Memory Ratio (Sequential2D / Sequential)')\n",
    "    ax4.set_title('Memory Efficiency')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e1cfe",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Sequential2D Features\n",
    "\n",
    "Let's explore some advanced features unique to Sequential2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4930ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sparse_connectivity():\n",
    "    \"\"\"\n",
    "    Show how Sequential2D can handle sparse connectivity patterns\n",
    "    \"\"\"\n",
    "    print(\"Creating Sequential2D with sparse connectivity...\")\n",
    "    \n",
    "    # Create a network with sparse connections using MaskedLinear\n",
    "    in_features_list = [100, 50, 25]\n",
    "    out_features_list = [50, 25, 10]\n",
    "    \n",
    "    # Use configuration-based creation with sparse blocks\n",
    "    config = {\n",
    "        \"in_features_list\": in_features_list,\n",
    "        \"out_features_list\": out_features_list,\n",
    "        \"block_types\": [\n",
    "            ['MaskedLinear', None, None],\n",
    "            ['MaskedLinear', 'MaskedLinear', None],\n",
    "            [None, 'MaskedLinear', 'Linear']\n",
    "        ],\n",
    "        \"block_kwargs\": [\n",
    "            [{'sparsity': 0.5}, None, None],\n",
    "            [{'sparsity': 0.3}, {'sparsity': 0.7}, None], \n",
    "            [None, {'sparsity': 0.2}, None]\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        sparse_net = Sequential2D.from_config(config)\n",
    "        \n",
    "        # Count parameters and sparsity\n",
    "        total_params = sum(p.numel() for p in sparse_net.parameters())\n",
    "        total_weights = sum(p.numel() for name, p in sparse_net.named_parameters() if 'weight' in name)\n",
    "        \n",
    "        print(f\"Sparse network created successfully!\")\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        test_input = torch.randn(16, sum(in_features_list))\n",
    "        output = sparse_net(test_input)\n",
    "        print(f\"Input shape: {test_input.shape}\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Sparse connectivity requires proper MaskedLinear configuration\")\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "        # Create a simpler version without sparsity constraints\n",
    "        blocks = [\n",
    "            [nn.Linear(100, 50), None, None],\n",
    "            [nn.Linear(50, 50), nn.Linear(50, 25), None],\n",
    "            [None, nn.Linear(25, 25), nn.Linear(25, 10)]\n",
    "        ]\n",
    "        \n",
    "        sparse_net = Sequential2D(in_features_list, out_features_list, blocks)\n",
    "        test_input = torch.randn(16, sum(in_features_list))\n",
    "        output = sparse_net(test_input)\n",
    "        \n",
    "        print(f\"Alternative network created:\")\n",
    "        print(f\"Input shape: {test_input.shape}\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in sparse_net.parameters())}\")\n",
    "\n",
    "demonstrate_sparse_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f33e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_analysis():\n",
    "    \"\"\"\n",
    "    Detailed memory usage analysis\n",
    "    \"\"\"\n",
    "    print(\"Memory Usage Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create networks of different complexities\n",
    "    configurations = [\n",
    "        (\"Simple Chain\", [784], [10], [[nn.Linear(784, 10)]]),\n",
    "        (\"Two Layer\", [784, 128], [128, 10], \n",
    "         [[nn.Linear(784, 128), None], [None, nn.Linear(128, 10)]]),\n",
    "        (\"Complex Multi-path\", [784, 256], [256, 128, 10],\n",
    "         [[nn.Linear(784, 256), nn.Linear(784, 128), nn.Linear(784, 10)],\n",
    "          [None, nn.Linear(256, 128), nn.Linear(256, 10)]])\n",
    "    ]\n",
    "    \n",
    "    for name, in_feat, out_feat, blocks in configurations:\n",
    "        net = Sequential2D(in_feat, out_feat, blocks)\n",
    "        \n",
    "        # Calculate memory usage\n",
    "        param_count = sum(p.numel() for p in net.parameters())\n",
    "        param_memory = sum(p.numel() * p.element_size() for p in net.parameters())\n",
    "        \n",
    "        # Test with different batch sizes\n",
    "        batch_sizes = [1, 16, 64, 256]\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Parameters: {param_count:,}\")\n",
    "        print(f\"  Parameter memory: {param_memory / 1024:.2f} KB\")\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            test_input = torch.randn(batch_size, sum(in_feat))\n",
    "            \n",
    "            # Measure peak memory usage during forward pass\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                output = net(test_input)\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "                print(f\"  Batch {batch_size:3d}: {peak_memory:.2f} MB peak GPU memory\")\n",
    "            else:\n",
    "                output = net(test_input)\n",
    "                activation_memory = output.numel() * output.element_size() / 1024  # KB\n",
    "                print(f\"  Batch {batch_size:3d}: ~{activation_memory:.2f} KB activation memory\")\n",
    "\n",
    "memory_usage_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc48eb",
   "metadata": {},
   "source": [
    "## Part 5: Use Case Recommendations\n",
    "\n",
    "Based on our analysis, here are guidelines for choosing between Sequential and Sequential2D:\n",
    "\n",
    "### Use PyTorch Sequential when:\n",
    "- Building simple linear architectures (input → hidden → output)\n",
    "- Rapid prototyping with standard layer sequences\n",
    "- Maximum simplicity and minimal code overhead\n",
    "- Working with pre-built layer combinations\n",
    "\n",
    "### Use Sequential2D when:\n",
    "- Complex connectivity patterns (skip connections, multi-path)\n",
    "- Sparse or structured connections\n",
    "- Research into iterative/dynamical systems\n",
    "- Block-structured architectures\n",
    "- Need for flexible input/output dimensions\n",
    "\n",
    "### Performance Considerations:\n",
    "- Sequential2D has slight overhead for simple chains\n",
    "- Sequential2D excels with complex patterns that would require custom modules\n",
    "- Memory usage is similar for equivalent architectures\n",
    "- Sequential2D provides more flexibility at the cost of complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_comparison_summary():\n",
    "    \"\"\"\n",
    "    Create a final comparison summary\n",
    "    \"\"\"\n",
    "    print(\"SEQUENTIAL vs SEQUENTIAL2D: FINAL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Aspect': [\n",
    "            'Simplicity',\n",
    "            'Linear Architectures', \n",
    "            'Complex Connectivity',\n",
    "            'Skip Connections',\n",
    "            'Multi-path Processing',\n",
    "            'Sparse Connections',\n",
    "            'Memory Efficiency',\n",
    "            'Computational Speed',\n",
    "            'Code Readability',\n",
    "            'Flexibility',\n",
    "            'Research Applications',\n",
    "            'Production Deployment'\n",
    "        ],\n",
    "        'Sequential': [\n",
    "            '★★★★★',\n",
    "            '★★★★★',\n",
    "            '★☆☆☆☆',\n",
    "            '★★☆☆☆',\n",
    "            '★☆☆☆☆',\n",
    "            '★☆☆☆☆',\n",
    "            '★★★★☆',\n",
    "            '★★★★★',\n",
    "            '★★★★★',\n",
    "            '★★☆☆☆',\n",
    "            '★★☆☆☆',\n",
    "            '★★★★★'\n",
    "        ],\n",
    "        'Sequential2D': [\n",
    "            '★★★☆☆',\n",
    "            '★★★★☆',\n",
    "            '★★★★★',\n",
    "            '★★★★★', \n",
    "            '★★★★★',\n",
    "            '★★★★★',\n",
    "            '★★★★☆',\n",
    "            '★★★★☆',\n",
    "            '★★★☆☆',\n",
    "            '★★★★★',\n",
    "            '★★★★★',\n",
    "            '★★★☆☆'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n★★★★★ = Excellent\")\n",
    "    print(f\"★★★★☆ = Very Good\") \n",
    "    print(f\"★★★☆☆ = Good\")\n",
    "    print(f\"★★☆☆☆ = Fair\")\n",
    "    print(f\"★☆☆☆☆ = Limited\")\n",
    "\n",
    "final_comparison_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be7c9f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the key differences between PyTorch's Sequential container and our Sequential2D implementation:\n",
    "\n",
    "1. **Functional Equivalence**: Sequential2D can replicate Sequential behavior when configured appropriately\n",
    "2. **Enhanced Capabilities**: Sequential2D supports complex connectivity patterns impossible with Sequential\n",
    "3. **Performance Trade-offs**: Sequential2D has minimal overhead while providing significantly more flexibility\n",
    "4. **Use Case Selection**: Choose based on architectural complexity requirements\n",
    "\n",
    "Sequential2D represents a powerful tool for researchers and practitioners working with complex neural architectures, particularly in the context of iterative networks and dynamical systems. While PyTorch's Sequential remains ideal for simple linear architectures, Sequential2D opens up new possibilities for innovative network designs.\n",
    "\n",
    "### Key Takeaways:\n",
    "- Both containers have their place in the deep learning toolkit\n",
    "- Sequential2D's flexibility comes with modest complexity overhead\n",
    "- Performance differences are minimal for equivalent architectures\n",
    "- Sequential2D excels in research contexts requiring architectural innovation\n",
    "- The choice depends on specific use case requirements and complexity needs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterativennsimple-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
