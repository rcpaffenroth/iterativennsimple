{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code if you are running on Google Colab\n",
    "# ! pip install https://github.com/rcpaffenroth/generatedata\n",
    "# ! pip install https://github.com/rcpaffenroth/iterativennsimple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579e155",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rcpaffenroth/iterativennsimple/blob/main/notebooks/6-rcp-Sequential-vs-Sequential2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4475020",
   "metadata": {},
   "source": [
    "# Sequential vs Sequential2D Comparison\n",
    "\n",
    "This notebook compares PyTorch's built-in Sequential container with our custom Sequential2D container, demonstrating:\n",
    "\n",
    "1. **Functional Equivalence**: Cases where both produce identical results\n",
    "2. **Architectural Differences**: Unique capabilities of Sequential2D\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the differences between linear sequential architectures and 2D block architectures\n",
    "- Learn when Sequential2D provides advantages over standard Sequential\n",
    "- Analyze performance trade-offs in different scenarios\n",
    "- Gain insights into advanced neural network architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba40f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from iterativennsimple.Sequential2D import Sequential2D\n",
    "from iterativennsimple.Sequential1D import Sequential1D\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")  # Force CPU for this example\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828cc19",
   "metadata": {},
   "source": [
    "## Part 1: Functional Equivalence\n",
    "\n",
    "Let's start by showing how Sequential2D can replicate the behavior of PyTorch's Sequential container.\n",
    "\n",
    "### Understanding the Key Difference\n",
    "\n",
    "- **Sequential**: A linear chain where data flows through layers one by one\n",
    "- **Sequential2D**: A 2D block matrix where data can flow through multiple parallel paths\n",
    "\n",
    "The magic is that when Sequential2D is configured properly, it can be mathematically equivalent to Sequential!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c35e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical number of parameters: 118282\n",
      "Network architectures created successfully!\n",
      "Sequential parameters: 118282\n",
      "Sequential2D parameters: 118282\n"
     ]
    }
   ],
   "source": [
    "def create_equivalent_networks():\n",
    "    \"\"\"\n",
    "    Create functionally equivalent networks using Sequential and Sequential2D\n",
    "    \n",
    "    This function demonstrates how to build the same 3-layer neural network\n",
    "    using both PyTorch's Sequential and our custom Sequential2D approach.\n",
    "    \"\"\"\n",
    "    # Network dimensions - typical sizes for MNIST-like problems\n",
    "    input_size = 784  # Flattened 28x28 image\n",
    "    hidden_size = 128 # Hidden layer size\n",
    "    output_size = 10  # Number of classes\n",
    "    \n",
    "    # Create the three linear layers that both networks will use\n",
    "    # This is where the linear algebra happens in our neural network \n",
    "    # and dynamical systems.\n",
    "    f1 = nn.Linear(input_size, hidden_size)   # First layer: 784 -> 128\n",
    "    f2 = nn.Linear(hidden_size, hidden_size)  # Second layer: 128 -> 128  \n",
    "    f3 = nn.Linear(hidden_size, output_size)  # Output layer: 128 -> 10\n",
    "\n",
    "    # This is the standard way: layers are applied one after another\n",
    "    # Data flow: input -> f1 -> ReLU -> f2 -> ReLU -> f3 -> output\n",
    "    sequential_net = nn.Sequential(\n",
    "        f1,         # Apply first linear transformation\n",
    "        nn.ReLU(),  # Apply activation function\n",
    "        f2,         # Apply second linear transformation\n",
    "        nn.ReLU(),  # Apply activation function\n",
    "        f3          # Apply final linear transformation (no activation)\n",
    "    )\n",
    "\n",
    "    # Sequential2D thinks of the network as a 2D block matrix that is iterated\n",
    "    \n",
    "    # Define the dimensions at each step\n",
    "    in_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "    out_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "        \n",
    "    # Sequential1D blocks: wrap regular PyTorch layers for use in Sequential2D\n",
    "    # Note: Sequential1D really just a wrapper for nn.Sequential that provides\n",
    "    # the size information needed by Sequential2D.\n",
    "    # Note: ReLU activations are placed BEFORE the linear layers in Sequential2D\n",
    "    # but they could just as well be afterward.\n",
    "    # This is  what we use\n",
    "    F1 = Sequential1D(nn.Sequential(f1),            # Just f1, no activation\n",
    "                      in_features=input_size,  out_features=hidden_size)\n",
    "    F2 = Sequential1D(nn.Sequential(nn.ReLU(), f2), # ReLU then f2\n",
    "                      in_features=hidden_size, out_features=hidden_size)\n",
    "    F3 = Sequential1D(nn.Sequential(nn.ReLU(), f3), # ReLU then f3\n",
    "                      in_features=hidden_size, out_features=output_size)\n",
    "    # But, you could also do it the other way around:\n",
    "    # F1 = Sequential1D(nn.Sequential(f1, nn.ReLU()),  # f1 *then* ReLU, the order matters!\n",
    "    #                   in_features=input_size, out_features=hidden_size)\n",
    "    # F2 = Sequential1D(nn.Sequential(f2, nn.ReLU()),  # f2 *then* ReLU\n",
    "    #                   in_features=hidden_size, out_features=hidden_size)\n",
    "    # F3 = Sequential1D(nn.Sequential(f3),             # No activation here!\n",
    "    #                   in_features=hidden_size, out_features=output_size)\n",
    "\n",
    "    # This matrix defines how data flows through the dynamical system\n",
    "    # NOTE: The structure istransposed compared to typical matrix notation! That is what Pytorch expects,\n",
    "    # since deep learning used *left* dot products. See https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    #\n",
    "    # Layer 0:   [None, F1,   None, None ]  <- Input layer\n",
    "    # Layer 1:   [None, None, F2,   None ]  <- Hidden layer  \n",
    "    # Layer 2:   [None, None, None, F3   ]  <- Output layer\n",
    "    # Layer 3:   [None, None, None, None ]  <- (unused, but makes operaor square, which is required)\n",
    "    # Note: in Sequential2D, a None block is \"free\" and does not perform any operation.,\n",
    "    blocks = [[None,   F1, None, None],\n",
    "              [None, None, F2,   None],\n",
    "              [None, None, None, F3],\n",
    "              [None, None, None, None]]\n",
    "    \n",
    "    # Calculate expected number of parameters for verification\n",
    "    W_parameters = input_size * hidden_size + hidden_size * hidden_size + hidden_size * output_size \n",
    "    b_parameters = hidden_size + hidden_size + output_size\n",
    "    print(f\"Theoretical number of parameters: {W_parameters + b_parameters}\")\n",
    "    \n",
    "    # Create the Sequential2D network\n",
    "    sequential2d_net = Sequential2D(in_features_list, out_features_list, blocks)    \n",
    "    return sequential_net, sequential2d_net\n",
    "\n",
    "# Create equivalent networks\n",
    "seq_net, seq2d_net = create_equivalent_networks()\n",
    "\n",
    "print(\"Network architectures created successfully!\")\n",
    "print(f\"Sequential parameters: {sum(p.numel() for p in seq_net.parameters())}\")\n",
    "print(f\"Sequential2D parameters: {sum(p.numel() for p in seq2d_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc52be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between outputs: 0.00e+00\n",
      "Outputs are identical\n",
      "âœ“ SUCCESS: Both networks produce the same results!\n"
     ]
    }
   ],
   "source": [
    "def test_equivalence():\n",
    "    \"\"\"\n",
    "    Test that both networks produce identical outputs\n",
    "    \n",
    "    This function demonstrates that Sequential and Sequential2D can produce\n",
    "    exactly the same results when configured properly. This is important\n",
    "    because it validates our Sequential2D implementation.\n",
    "    \"\"\"\n",
    "    # Create test input - random data simulating a batch of flattened images\n",
    "    batch_size = 32\n",
    "    input_size = 784\n",
    "    test_input = torch.randn(batch_size, input_size)\n",
    "    \n",
    "    # Get outputs from both networks\n",
    "    with torch.no_grad():  # No gradients needed for testing\n",
    "        # ===== Sequential1D neural network =====\n",
    "        # Just call the network directly, since the layers are internal\n",
    "        seq_output = seq_net(test_input)\n",
    "\n",
    "        # ===== Sequential2D dynamical system =====\n",
    "        # Requires iterative application\n",
    "        # Start with input in first position, None elsewhere\n",
    "        seq2d_output = [test_input, None, None, None]\n",
    "\n",
    "        # The key insight: iterate the Sequential2D function multiple times\n",
    "        # Each iteration applies one \"time step\" of the block matrix\n",
    "        # This is equivalent to applying layers sequentially!\n",
    "        for i in range(3):  # 3 iterations for the equivalent of 3 \"layers\"\n",
    "            seq2d_output = seq2d_net(seq2d_output)\n",
    "            \n",
    "    # ===== VERIFICATION =====\n",
    "    # Check if outputs are identical (within numerical precision)\n",
    "    # The \"output\" of Sequential2D is a list - we want the final element\n",
    "    max_diff = torch.max(torch.abs(seq_output - seq2d_output[3])).item()\n",
    "    \n",
    "    print(f\"Maximum difference between outputs: {max_diff:.2e}\")\n",
    "    print(f\"Outputs are {'identical' if max_diff < 1e-6 else 'different'}\")\n",
    "    \n",
    "    if max_diff < 1e-6:\n",
    "        print(\"âœ“ SUCCESS: Both networks produce the same results!\")\n",
    "    else:\n",
    "        print(\"âœ— WARNING: Networks produce different results!\")\n",
    "    \n",
    "    return seq_output, seq2d_output\n",
    "\n",
    "seq_output, seq2d_output = test_equivalence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10e4d4",
   "metadata": {},
   "source": [
    "## Part 2: Training Equivalence\n",
    "\n",
    "Now we'll test something more advanced: **do both networks train the same way?**\n",
    "\n",
    "This is a crucial test because:\n",
    "- We've shown they produce the same outputs (forward pass equivalence)\n",
    "- But do they also learn the same way during training? (backward pass equivalence)\n",
    "\n",
    "### Key Concept: Independent but Identical Networks\n",
    "\n",
    "To test training equivalence properly, we need to:\n",
    "1. Create two networks with **identical starting weights**\n",
    "2. Keep them **independent** (training one doesn't affect the other)\n",
    "3. Train them on the **same data** with the **same optimizer settings**\n",
    "4. Verify they learn identically\n",
    "\n",
    "This tests whether Sequential2D's gradient computation matches Sequential's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf34469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing training equivalence with 3-layer networks...\n",
      "Networks created with 118282 parameters each\n",
      "\n",
      "1. Before training - checking outputs are identical:\n",
      "   Output difference: 0.00e+00 (should be ~0)\n",
      "\n",
      "2. Training both networks for one step:\n",
      "   Sequential loss: 0.9985\n",
      "   Sequential2D loss: 0.9985\n",
      "\n",
      "3. After training - checking if networks still behave similarly:\n",
      "   Output difference after training: 0.00e+00\n",
      "   Loss difference: 0.00e+00\n",
      "\n",
      "âœ“ SUCCESS: Both networks computed identical losses!\n",
      "  This proves Sequential2D's gradient computation is correct.\n",
      "\n",
      "âœ“ Training test complete! Both 3-layer networks train equivalently.\n"
     ]
    }
   ],
   "source": [
    "def test_training_equivalence():\n",
    "    \"\"\"\n",
    "    Test that Sequential and Sequential2D train identically using the same 3-layer model\n",
    "    \n",
    "    This is the most important test! It verifies that Sequential2D not only\n",
    "    produces the same outputs, but also computes gradients and updates weights\n",
    "    in exactly the same way as Sequential during training.\n",
    "    \"\"\"\n",
    "    print(\"Testing training equivalence with 3-layer networks...\")\n",
    "    \n",
    "    # Use the same architecture as our equivalence test\n",
    "    input_size = 784\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    \n",
    "    # ===== STEP 1: Create base transformations that both will use =====\n",
    "    # These will provide the initial random weights that both networks will copy\n",
    "    f1_base = nn.Linear(input_size, hidden_size)   # Will be randomly initialized\n",
    "    f2_base = nn.Linear(hidden_size, hidden_size)  # Will be randomly initialized\n",
    "    f3_base = nn.Linear(hidden_size, output_size)  # Will be randomly initialized\n",
    "    \n",
    "    # ===== STEP 2: Create Sequential neural network =====\n",
    "    # Make new layers and copy the base weights to them\n",
    "    f1_seq = nn.Linear(input_size, hidden_size)\n",
    "    f2_seq = nn.Linear(hidden_size, hidden_size)\n",
    "    f3_seq = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    # Critical: Copy weights and biases exactly\n",
    "    f1_seq.weight.data.copy_(f1_base.weight.data)  # Copy weight matrix\n",
    "    f1_seq.bias.data.copy_(f1_base.bias.data)      # Copy bias vector\n",
    "    f2_seq.weight.data.copy_(f2_base.weight.data)\n",
    "    f2_seq.bias.data.copy_(f2_base.bias.data)\n",
    "    f3_seq.weight.data.copy_(f3_base.weight.data)\n",
    "    f3_seq.bias.data.copy_(f3_base.bias.data)\n",
    "    \n",
    "    # Build the Sequential network\n",
    "    seq_net = nn.Sequential(f1_seq, nn.ReLU(), f2_seq, nn.ReLU(), f3_seq)\n",
    "\n",
    "    # ===== STEP 3: Create Sequential2D dynamical system =====\n",
    "    # Make another set of new layers and copy the SAME base weights\n",
    "    f1_seq2d = nn.Linear(input_size, hidden_size)\n",
    "    f2_seq2d = nn.Linear(hidden_size, hidden_size)\n",
    "    f3_seq2d = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    # Copy the exact same initial weights (identical starting point!)\n",
    "    f1_seq2d.weight.data.copy_(f1_base.weight.data)\n",
    "    f1_seq2d.bias.data.copy_(f1_base.bias.data)\n",
    "    f2_seq2d.weight.data.copy_(f2_base.weight.data)\n",
    "    f2_seq2d.bias.data.copy_(f2_base.bias.data)\n",
    "    f3_seq2d.weight.data.copy_(f3_base.weight.data)\n",
    "    f3_seq2d.bias.data.copy_(f3_base.bias.data)\n",
    "    \n",
    "    # Build Sequential2D using the same block matrix structure as before\n",
    "    in_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "    out_features_list = [input_size, hidden_size, hidden_size, output_size]\n",
    "    \n",
    "    F1 = Sequential1D(nn.Sequential(f1_seq2d), in_features=input_size, out_features=hidden_size)\n",
    "    F2 = Sequential1D(nn.Sequential(nn.ReLU(), f2_seq2d), in_features=hidden_size, out_features=hidden_size)\n",
    "    F3 = Sequential1D(nn.Sequential(nn.ReLU(), f3_seq2d), in_features=hidden_size, out_features=output_size)\n",
    "    \n",
    "    blocks = [[None, F1,   None, None],\n",
    "              [None, None, F2,   None],\n",
    "              [None, None, None, F3],\n",
    "              [None, None, None, None]]\n",
    "    \n",
    "    seq2d_net = Sequential2D(in_features_list, out_features_list, blocks)\n",
    "    \n",
    "    print(f\"Networks created with {sum(p.numel() for p in seq_net.parameters())} parameters each\")\n",
    "\n",
    "    # ===== STEP 4: Prepare training data =====\n",
    "    batch_size = 32\n",
    "    x = torch.randn(batch_size, input_size)      # Random input data\n",
    "    target = torch.randn(batch_size, output_size) # Random target data\n",
    "    \n",
    "    print(\"\\n1. Before training - checking outputs are identical:\")\n",
    "    with torch.no_grad():\n",
    "        # Sequential forward pass\n",
    "        out_seq = seq_net(x)\n",
    "        \n",
    "        # Sequential2D forward pass (using the iterative method)\n",
    "        seq2d_input = [x, None, None, None]\n",
    "        for i in range(3):  # 3 iterations for 3 effective layers\n",
    "            seq2d_input = seq2d_net(seq2d_input)\n",
    "        out_seq2d = seq2d_input[3]  # Extract final output\n",
    "        \n",
    "        diff = torch.max(torch.abs(out_seq - out_seq2d))\n",
    "        print(f\"   Output difference: {diff:.2e} (should be ~0)\")\n",
    "    \n",
    "    print(\"\\n2. Training both networks for one step:\")\n",
    "    criterion = nn.MSELoss()  # Mean squared error loss\n",
    "    \n",
    "    # ===== Train Sequential neural network =====\n",
    "    optimizer_seq = torch.optim.SGD(seq_net.parameters(), lr=0.01)\n",
    "    optimizer_seq.zero_grad()          # Clear any existing gradients\n",
    "    loss_seq = criterion(seq_net(x), target)  # Compute loss\n",
    "    loss_seq.backward()                # Compute gradients via backpropagation\n",
    "    optimizer_seq.step()               # Update weights\n",
    "    print(f\"   Sequential loss: {loss_seq.item():.4f}\")\n",
    "\n",
    "    # ===== Train Sequential2D network =====\n",
    "    optimizer_seq2d = torch.optim.SGD(seq2d_net.parameters(), lr=0.01)\n",
    "    optimizer_seq2d.zero_grad()        # Clear any existing gradients\n",
    "    # Forward pass through Sequential2D\n",
    "    seq2d_input = [x, None, None, None]\n",
    "    # Note, the 3 iterations are important!\n",
    "    # It makes the dynamical system equivalent to the 3 layers in the Sequential network\n",
    "    for i in range(3):\n",
    "        seq2d_input = seq2d_net(seq2d_input)\n",
    "    loss_seq2d = criterion(seq2d_input[3], target)  # Compute loss\n",
    "    loss_seq2d.backward()              # Compute gradients via backpropagation\n",
    "    optimizer_seq2d.step()             # Update weights\n",
    "    print(f\"   Sequential2D loss: {loss_seq2d.item():.4f}\")\n",
    "    \n",
    "    print(\"\\n3. After training - checking if networks still behave similarly:\")\n",
    "    with torch.no_grad():\n",
    "        # Test on new data to see if both networks learned similarly\n",
    "        test_x = torch.randn(16, input_size)\n",
    "        \n",
    "        out_seq_after = seq_net(test_x)\n",
    "        \n",
    "        seq2d_test = [test_x, None, None, None]\n",
    "        for i in range(3):\n",
    "            seq2d_test = seq2d_net(seq2d_test)\n",
    "        out_seq2d_after = seq2d_test[3]\n",
    "        \n",
    "        diff_after = torch.max(torch.abs(out_seq_after - out_seq2d_after))\n",
    "        print(f\"   Output difference after training: {diff_after:.2e}\")\n",
    "        print(f\"   Loss difference: {abs(loss_seq.item() - loss_seq2d.item()):.2e}\")\n",
    "    \n",
    "    # ===== Interpretation =====\n",
    "    if abs(loss_seq.item() - loss_seq2d.item()) < 1e-6:\n",
    "        print(f\"\\nâœ“ SUCCESS: Both networks computed identical losses!\")\n",
    "        print(f\"  This proves Sequential2D's gradient computation is correct.\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— WARNING: Networks computed different losses!\")\n",
    "        print(f\"  This suggests a bug in Sequential2D's implementation.\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Training test complete! Both 3-layer networks train equivalently.\")\n",
    "\n",
    "# Run the training test\n",
    "test_training_equivalence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff06a1",
   "metadata": {},
   "source": [
    "## Part 3: Unique Capabilities of Sequential2D\n",
    "\n",
    "Now let's explore scenarios where Sequential2D offers capabilities that standard Sequential **cannot** provide.\n",
    "\n",
    "### What Makes Sequential2D Special?\n",
    "\n",
    "Sequential2D isn't just another way to implement Sequential networks. It enables entirely new architectures:\n",
    "\n",
    "1. **Multiple Parallel Paths**: Data can flow through several paths simultaneously\n",
    "2. **Skip Connections**: Information can jump across multiple layers\n",
    "3. **Complex Connectivity**: Non-linear data flow patterns impossible with Sequential\n",
    "4. **Block Matrix Structure**: Think of your network as a 2D grid of computational blocks\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "These capabilities allow you to implement architectures like:\n",
    "- ResNet-style skip connections\n",
    "- Multi-path networks (like Inception)\n",
    "- Dynamical systems with feedback loops\n",
    "- Networks with complex information flow patterns\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc2686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_sequential2d():\n",
    "    \"\"\"\n",
    "    Create a Sequential2D network with complex connectivity patterns\n",
    "    that cannot be represented by standard Sequential\n",
    "    \n",
    "    This demonstrates the unique power of Sequential2D: creating networks\n",
    "    with multiple parallel paths and skip connections that would be\n",
    "    impossible to represent with PyTorch's Sequential container.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a complex connectivity pattern using configuration\n",
    "    # This creates a network with:\n",
    "    # - Multiple input paths\n",
    "    # - Skip connections between non-adjacent layers  \n",
    "    # - Parallel processing paths\n",
    "    # - Complex aggregation patterns\n",
    "    cfg = { \n",
    "        'in_features_list': [50, 100, 200, 150],   # Input sizes for each \"layer\"\n",
    "        'out_features_list': [100, 200, 150, 10], # Output sizes for each \"layer\"\n",
    "        'block_types': [\n",
    "            # Layer 0 -> Layer 1,2:   Two parallel paths from input\n",
    "            ['Linear', 'Linear', None,     None],\n",
    "            # Layer 1 -> Layer 2,3,4: Three parallel paths from first hidden layer  \n",
    "            [None,     'Linear', 'Linear', 'Linear'],\n",
    "            # Layer 2 -> Layer 3,4:   Two paths from second hidden layer\n",
    "            [None,     None,     'Linear', 'Linear'],\n",
    "            # Layer 3 -> Layer 4:     Single path to output\n",
    "            [None,     None,     None,     'Linear']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    complex_net = Sequential2D.from_config(cfg)\n",
    "        \n",
    "    return complex_net\n",
    "\n",
    "complex_net = create_complex_sequential2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53962ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Method 1: Single Tensor Input ===\n",
      "Input shape: torch.Size([16, 500])\n",
      "Final output shape: torch.Size([16, 460])\n",
      "\n",
      "=== Method 2: List-Based Input ===\n",
      "Now we'll provide input only to the first blocks and see what happens...\n",
      "Input list:\n",
      "  Path 0: shape torch.Size([16, 50])\n",
      "  Path 1: None (no input)\n",
      "  Path 2: None (no input)\n",
      "  Path 3: None (no input)\n",
      "\n",
      "Outputs from each path:\n",
      "  Path 0: shape torch.Size([16, 100])\n",
      "  Path 1: shape torch.Size([16, 200])\n",
      "  Path 2: None (no output)\n",
      "  Path 3: None (no output)\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_complex_forward():\n",
    "    \"\"\"\n",
    "    Demonstrate the complex forward pass of Sequential2D\n",
    "    \n",
    "    This shows how Sequential2D can handle both:\n",
    "    1. Standard tensor input (automatic splitting across paths)\n",
    "    2. List-based input (manual control over each path)\n",
    "    \n",
    "    Understanding both modes is crucial for using Sequential2D effectively.\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "    \n",
    "    # ===== METHOD 1: SINGLE TENSOR INPUT =====\n",
    "    # Sequential2D automatically splits the input across different paths\n",
    "    # Total input size = sum of all input feature sizes\n",
    "    total_input_size = 50 + 100 + 200 + 150  # = 500\n",
    "    test_input = torch.randn(batch_size, total_input_size)\n",
    "    \n",
    "    print(\"=== Method 1: Single Tensor Input ===\")\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    \n",
    "    # Forward pass through complex network\n",
    "    output = complex_net(test_input)\n",
    "    print(f\"Final output shape: {output.shape}\")\n",
    "    \n",
    "    # ===== METHOD 2: LIST-BASED INPUT =====\n",
    "    # Manually specify input for each path (more control)\n",
    "    print(\"\\n=== Method 2: List-Based Input ===\")\n",
    "    print(\"Now we'll provide input only to the first blocks and see what happens...\")\n",
    "    \n",
    "    # Only provide input to first path, None elsewhere\n",
    "    input_list = [torch.randn(batch_size, 50), None, None, None]\n",
    "    print(\"Input list:\")\n",
    "    for i, inp in enumerate(input_list):\n",
    "        if inp is not None:\n",
    "            print(f\"  Path {i}: shape {inp.shape}\")\n",
    "        else:\n",
    "            print(f\"  Path {i}: None (no input)\")\n",
    "    \n",
    "    # Use the special forward_list method for list inputs\n",
    "    output_list = complex_net.forward_list(input_list)\n",
    "    \n",
    "    print(f\"\\nOutputs from each path:\")\n",
    "    for i, out in enumerate(output_list):\n",
    "        if out is not None:\n",
    "            print(f\"  Path {i}: shape {out.shape}\")\n",
    "        else:\n",
    "            print(f\"  Path {i}: None (no output)\")\n",
    "        \n",
    "    return output, output_list\n",
    "\n",
    "output, output_list = demonstrate_complex_forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterativennsimple-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
